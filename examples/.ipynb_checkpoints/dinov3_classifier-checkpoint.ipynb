{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2110fa2-07ae-4c6f-ab24-a9e8517b5e56",
   "metadata": {},
   "source": [
    "## Environment preparation\n",
    "### Install dependencies\n",
    "* conda create -n dino_env python=3.10.19\n",
    "* pip install -r requirements.txt\n",
    "```\n",
    "cmake==4.1.0\n",
    "datasets==3.6.0\n",
    "dill==0.3.8\n",
    "distlib==0.3.4\n",
    "hf_transfer==0.1.9\n",
    "isoduration==20.11.0\n",
    "json5==0.12.1\n",
    "jsonlines==4.0.0\n",
    "jsonpointer==3.0.0\n",
    "jsonschema==4.25.1\n",
    "jsonschema-specifications==2025.9.1\n",
    "matplotlib==3.10.0\n",
    "matplotlib-inline==0.1.7\n",
    "multidict==6.6.4\n",
    "multiprocess==0.70.16\n",
    "numpy==1.26.0\n",
    "opencv-python\n",
    "pandas==2.2.3\n",
    "pillow==12.0.0\n",
    "PyQt5==5.15.6\n",
    "PyQt5-sip==12.9.1\n",
    "safetensors==0.6.2\n",
    "scikit-image==0.25.2\n",
    "scikit-learn==1.7.2\n",
    "scipy==1.15.3\n",
    "tensorboard==2.15.2\n",
    "tensorboard-data-server==0.7.2\n",
    "threadpoolctl==3.6.0\n",
    "timm==1.0.24\n",
    "torch==2.9.1\n",
    "transformers[torch]==4.57.6\n",
    "```\n",
    "* conda install ipykernel\n",
    "* python -m ipykernel install --user --name=dino_env --display-name=\"Python (dino_env)\"\n",
    "\n",
    "### HuggingFace Mirror setup\n",
    "* open ~/.bashrc, add:\n",
    "```\n",
    "export HF_ENDPOINT=\"https://hf-mirror.com\"\n",
    "export HF_HOME=\"/home/your_path/huggingface\" # huggingface dataset cache path\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbaf0c0-a117-4be7-ad27-028d8698b6b3",
   "metadata": {},
   "source": [
    "## DinoV3 Intro\n",
    "DINOv3 is a Vision Transformer (ViT) model, specifically a large-scale, self-supervised one, that uses transformer architecture for learning powerful visual features, though it also incorporates innovations like register tokens and offers variants like ConvNeXt for broader use. Its backbone is a massive ViT (e.g., ViT-7B), trained to understand images without human labels, making it a state-of-the-art foundation model for various computer vision task\n",
    "### Key aspects\n",
    "* Transformer Backbone: DINOv3 models utilize the Vision Transformer architecture, breaking images into patches and processing them through transformer layers.\n",
    "* Self-Supervised Learning: It learns by solving puzzles on unlabeled data (like predicting missing parts of an image), a technique that excels with transformers.\n",
    "* Architectural Enhancements: It adds features like Axial Rotary Positional Embeddings (RoPE) and register tokens to improve performance, but the core remains transformer-based.\n",
    "\n",
    "## Model Train Procedure\n",
    "* define model\n",
    "* prepare dataset\n",
    "* create dataloader\n",
    "* train\n",
    "* test (inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69838480-f79f-4cb8-8c7a-aa257e11b3fc",
   "metadata": {},
   "source": [
    "### _Define DinoV3 + cls_head_\n",
    "* use timm module to access dinov3 model\n",
    "  * timm/vit_small_patch16_dinov3.lvd1689m\n",
    "  * timm/vit_large_patch16_dinov3.lvd1689m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d8d1d120-f033-494f-b3f9-29b59a230dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone number of features: 384, backbone_size is: 21.586944M, total_size is: 22.509062M\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import timm\n",
    "\n",
    "# timm/vit_small_patch16_dinov3.lvd1689m\n",
    "# timm/vit_large_patch16_dinov3.lvd1689m\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class DinoClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, hidden_dim=1024):\n",
    "        super(DinoClassifier, self).__init__()\n",
    "        # Load pretrained model\n",
    "        self.backbone = timm.create_model('timm/vit_small_patch16_dinov3.lvd1689m', pretrained=True, num_classes=0)\n",
    "        self.backbone.eval()\n",
    "        self.num_classes = num_classes\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requres_grad = False\n",
    "\n",
    "        # create simple classification head\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(self.backbone.num_features, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.1),  # 0.1 is the probability to randomly zero-out input tensor element. Only used during training, not eval\n",
    "            nn.Linear(hidden_dim, hidden_dim//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(hidden_dim//2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        output = self.head(features)\n",
    "        return output, features\n",
    "\n",
    "    def get_info(self):\n",
    "        backbone_size = sum(param.numel() for param in self.backbone.parameters())\n",
    "        head_size = sum(param.numel() for param in self.head.parameters())\n",
    "        total_size = backbone_size + head_size\n",
    "        feature_dim = self.backbone.num_features\n",
    "        print(f\"backbone number of features: {feature_dim}, backbone_size is: {backbone_size/1e6}M, total_size is: {total_size/1e6}M\")\n",
    "\n",
    "num_classes = 6\n",
    "custom_model = DinoClassifier(num_classes)\n",
    "custom_model.get_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44dc9c0-2aaa-4f83-bc55-1272f44b0ec1",
   "metadata": {},
   "source": [
    "### _Prepare dataset_\n",
    "```\n",
    "/home/yang/MyRepos/tensorRT/datasets/port_actibot/\n",
    "└── images/\n",
    "│   ├── train/\n",
    "│   │   ├── 1766394897.975832.jpg\n",
    "│   │   └── ...\n",
    "│   └── val/\n",
    "│       └── 1766396065.439413.jpg\n",
    "│       └── ...\n",
    "└── labels/\n",
    "    ├── train/\n",
    "    │   ├── 1766394897.975832.txt\n",
    "    │   └── ...\n",
    "    └── val/\n",
    "        └── 1766396065.439413.txt\n",
    "        └── ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5ea55ee7-f355-4433-b6ec-51034a1b7b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image, ImageOps\n",
    "import os\n",
    "import random\n",
    "\n",
    "def create_file_list(root_dir):\n",
    "    dir_contents = os.listdir(root_dir)\n",
    "    path_list = [os.path.join(root_dir, file_path) for file_path in dir_contents]\n",
    "\n",
    "    return path_list\n",
    "\n",
    "def get_transform(model_name):\n",
    "    model = timm.create_model(model_name, pretrained=True, num_classes=0)\n",
    "    data_config = timm.data.resolve_model_data_config(model)\n",
    "    print(data_config)\n",
    "    transform = timm.data.create_transform(\n",
    "        **data_config,\n",
    "        is_training = False\n",
    "    ) # create_transform: https://zhuanlan.zhihu.com/p/638454986\n",
    "    return transform\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    @classmethod\n",
    "    def fromDirectory(cls, data_dir, label_dir, transform = None):\n",
    "        file_list = create_file_list(data_dir)\n",
    "        data = []\n",
    "        labels = []\n",
    "        for file in file_list:\n",
    "            img = Image.open(file).convert('RGB')\n",
    "            data.append(img)\n",
    "            label_file = file.replace(data_dir, label_dir).replace('.jpg', '.txt')\n",
    "            with open(label_file, 'r') as f:\n",
    "                label = int(f.read().strip())\n",
    "                labels.append(label)\n",
    "        if transform:\n",
    "            data = [transform(img) for img in data]\n",
    "        return cls(data, labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        return sample, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dc8db854-940c-4e17-a6aa-3646a2eb2d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_size': (3, 256, 256), 'interpolation': 'bicubic', 'mean': (0.485, 0.456, 0.406), 'std': (0.229, 0.224, 0.225), 'crop_pct': 1.0, 'crop_mode': 'center'}\n",
      "train dataset size: 1898 val dataset size: 132\n"
     ]
    }
   ],
   "source": [
    "transforms = get_transform('timm/vit_large_patch16_dinov3.lvd1689m')\n",
    "train_file_directory = \"/home/yang/MyRepos/tensorRT/datasets/port_actibot/images/train\"\n",
    "train_label_directory = \"/home/yang/MyRepos/tensorRT/datasets/port_actibot/labels/train\"\n",
    "val_file_directory = \"/home/yang/MyRepos/tensorRT/datasets/port_actibot/images/val\"\n",
    "val_label_directory = \"/home/yang/MyRepos/tensorRT/datasets/port_actibot/labels/val\"\n",
    "\n",
    "train_dataset = CustomDataset.fromDirectory(\n",
    "    train_file_directory,\n",
    "    train_label_directory,\n",
    "    transform = transforms\n",
    ")\n",
    "val_dataset = CustomDataset.fromDirectory(\n",
    "    val_file_directory, \n",
    "    val_label_directory,\n",
    "    transform = transforms,\n",
    ")\n",
    "print(f\"train dataset size: {int(train_dataset.__len__())} val dataset size: {int(val_dataset.__len__())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7421ba4c-7db7-4625-ade7-29c56c67793f",
   "metadata": {},
   "source": [
    "### _Create dataloader_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c39a31ab-985d-45f6-a26c-8c41185dc151",
   "metadata": {},
   "outputs": [],
   "source": [
    "_TRAIN_BATCH = 256\n",
    "_VAL_BATCH = 32\n",
    "_NUM_WORKERS = 12\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = _TRAIN_BATCH,\n",
    "    shuffle = True,\n",
    "    num_workers = _NUM_WORKERS\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size = _VAL_BATCH,\n",
    "    shuffle = True,\n",
    "    num_workers = _NUM_WORKERS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a083e440-e92d-42aa-8032-6f9a73ac9574",
   "metadata": {},
   "source": [
    "### _Train Model_\n",
    "* init model\n",
    "* define loss\n",
    "* define optimizer\n",
    "* define learning rate scheduler (optional)\n",
    "* train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e2a865-cd4c-473a-917f-15bd19d23c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device selected is: cuda\n",
      "random loss should be -1.791759469228055\n",
      "train iteration: 0 data size: 256\n",
      "train iteration: 1 data size: 256\n",
      "train iteration: 2 data size: 256\n",
      "train iteration: 3 data size: 256\n",
      "train iteration: 4 data size: 256\n",
      "train iteration: 5 data size: 256\n",
      "train iteration: 6 data size: 256\n",
      "train iteration: 7 data size: 106\n",
      "val iteration: 0 data size: 32\n",
      "val iteration: 1 data size: 32\n",
      "val iteration: 2 data size: 32\n",
      "val iteration: 3 data size: 32\n",
      "val iteration: 4 data size: 4\n",
      "Epoch [1/50], Train loss: 1.5443, Val loss: 1.4368\n",
      "Epoch [2/50], Train loss: 1.2564, Val loss: 1.0749\n",
      "Epoch [3/50], Train loss: 0.9338, Val loss: 0.8090\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "_NUM_EPOCH = 50\n",
    "_LR_MAX = 0.001\n",
    "_LR_MIN = 0.0002\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# init model\n",
    "num_classes = 6\n",
    "custom_model = DinoClassifier(num_classes)\n",
    "custom_model.to(device)\n",
    "print(f\"device selected is: {device}\")\n",
    "\n",
    "# loss, optimizer (adam or admaw), learning rate scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "cross_entropy_random = -math.log(1/6)\n",
    "print(f\"random loss should be {cross_entropy_random}\")\n",
    "optimizer = optim.Adam(custom_model.head.parameters(), lr=_LR_MAX)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=_NUM_EPOCH, eta_min=_LR_MIN)\n",
    "\n",
    "# train loop\n",
    "# train dataset size: 1898 val dataset size: 132\n",
    "for epoch in range(_NUM_EPOCH):\n",
    "    # train\n",
    "    custom_model.train()\n",
    "    running_loss = 0.0\n",
    "    counter = 0\n",
    "    for images, labels in train_loader:\n",
    "        if epoch < 1:\n",
    "            print(f\"train iteration: {counter} data size: {len(labels)}\")\n",
    "        counter += 1\n",
    "        # forward pass once\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = custom_model(images)\n",
    "        # calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        # back propagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    running_loss_train = running_loss / len(train_loader)\n",
    "\n",
    "    # eval\n",
    "    custom_model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        counter = 0\n",
    "        for images, labels in val_loader:\n",
    "            if epoch < 1:\n",
    "                print(f\"val iteration: {counter} data size: {len(labels)}\")\n",
    "            counter += 1\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs, _ = custom_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "    running_loss_val = running_loss / len(val_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{_NUM_EPOCH}], Train loss: {running_loss_train:.4f}, Val loss: {running_loss_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74a8f58-0484-4e0c-af04-84d539ca8802",
   "metadata": {},
   "source": [
    "### _Inference_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4e74d911-8dc7-48c5-bd00-f97de40a46b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yang/MyRepos/tensorRT/datasets/port_actibot/episode5/1766396306.723036.jpg class name is unplugged with confidence 0.9992132186889648\n",
      "/home/yang/MyRepos/tensorRT/datasets/port_actibot/episode5/1766396304.250854.jpg class name is port1 with confidence 0.9984049201011658\n",
      "/home/yang/MyRepos/tensorRT/datasets/port_actibot/episode5/1766396392.096162.jpg class name is port3 with confidence 0.9988846182823181\n",
      "/home/yang/MyRepos/tensorRT/datasets/port_actibot/episode5/1766396427.542294.jpg class name is port4 with confidence 0.9330883622169495\n",
      "/home/yang/MyRepos/tensorRT/datasets/port_actibot/episode5/1766396362.981309.jpg class name is port5 with confidence 0.9995386600494385\n"
     ]
    }
   ],
   "source": [
    "image_0 = \"/home/yang/MyRepos/tensorRT/datasets/port_actibot/episode5/1766396306.723036.jpg\"  # 0\n",
    "image_1 = \"/home/yang/MyRepos/tensorRT/datasets/port_actibot/episode5/1766396304.250854.jpg\"  # 1\n",
    "image_3 = \"/home/yang/MyRepos/tensorRT/datasets/port_actibot/episode5/1766396392.096162.jpg\"  # 3\n",
    "image_4 = \"/home/yang/MyRepos/tensorRT/datasets/port_actibot/episode5/1766396427.542294.jpg\"  # 4\n",
    "image_5 = \"/home/yang/MyRepos/tensorRT/datasets/port_actibot/episode5/1766396362.981309.jpg\"  # 5\n",
    "class_names=['unplugged', 'port1', 'port2', 'port3', 'port4', 'port5']\n",
    "\n",
    "def process_image(transforms, file_path):\n",
    "    image = Image.open(file_path).convert('RGB')\n",
    "    input_tensor = transforms(image).unsqueeze(0).to(device)\n",
    "    return input_tensor\n",
    "\n",
    "def predict(file_path, transforms):\n",
    "    custom_model.eval()\n",
    "    input_tensor = process_image(transforms, file_path)\n",
    "    with torch.no_grad():\n",
    "        output, features = custom_model.forward(input_tensor)\n",
    "        prob = torch.softmax(output, dim=1)\n",
    "        confidence, predicted = torch.max(prob, 1)\n",
    "    # print(confidence)\n",
    "    # print(predicted)\n",
    "    print(f\"{file_path} class name is {class_names[predicted.item()]} with confidence {confidence.item()}\")\n",
    "\n",
    "predict(image_0, transforms)\n",
    "predict(image_1, transforms)\n",
    "predict(image_3, transforms)\n",
    "predict(image_4, transforms)\n",
    "predict(image_5, transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217edbb7-abb7-44ed-bc26-8e18ee2237bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (monitor_env)",
   "language": "python",
   "name": "monitor_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
